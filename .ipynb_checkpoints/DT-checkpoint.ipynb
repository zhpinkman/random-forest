{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### at first we are importing the libraries used to generating, fitting and evaluating the decision tree model with the givven data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - 1 ) Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### reading the dataset and make two other datasets out of it: one for evaluating the final model and one for training the model and as it makes sense we have to omit the target column cause with the target included the model will be overfitted and the accuracy will be 100 % that is not really a good thing although it sounds good.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "Y = data.target\n",
    "X = data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### train test split method of scikit learn library will help us devide the whole dataset into training dataset and test data set with a custom ratio. for our example we are taking 80 % of the whole data for training the model and the rest for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### fiiting the model on our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### getting the prediction of the model about our testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### for the last part we are getting the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7213114754098361\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 ) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - 1 ) Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "Y = data.target\n",
    "X = data.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - 2 ) Bagging (Bootstrap aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### generating 5 list of samples, each with 150 samples in it. sample will help us taking samples of our dataset with no replacement; but 5 datasets can have common samples in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_train = [X_train.sample(n = 150) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_train = [y_train[x_train.index] for x_train in Xs_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [DecisionTreeClassifier() for i in range(5)]\n",
    "fitted_clfs = [clfs[i].fit(Xs_train[i], ys_train[i]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [fitted_clf.predict(X_test) for fitted_clf in fitted_clfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### here after getting the predictions for the test dataset for each Decision Tree we use voting for generating the final prediction on the test dataset. in this approach we get predict the target for each dataset with each Decision Tree and at the end we look at the targets predicted for each row of our dataset and decide the final target according to which is the most predicted target for that row and this way we can predict better using multiple Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "for i in range(len(X_test)):\n",
    "    ones = [pred[i] for pred in preds].count(1)\n",
    "    zeros = [pred[i] for pred in preds].count(0)\n",
    "    if zeros < ones:\n",
    "        final_preds.append(1)\n",
    "    else:\n",
    "        final_preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.7213114754098361\n"
     ]
    }
   ],
   "source": [
    "print(\"Bagging Accuracy:\",metrics.accuracy_score(y_test, final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - 3 ) Model with feature excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### generating decision tree with omiting selected column to check whether its existence in the dataset helps us making a better model or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision_tree_accuracy_without_column(column):\n",
    "    data = pd.read_csv('data.csv')\n",
    "    Y = data.target\n",
    "    X = data.drop(columns=['target', column])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy after removing column, age  : \n",
      "0.7049180327868853\n",
      "Decision Tree Accuracy after removing column, sex  : \n",
      "0.7049180327868853\n",
      "Decision Tree Accuracy after removing column, cp  : \n",
      "0.7213114754098361\n",
      "Decision Tree Accuracy after removing column, trestbps  : \n",
      "0.7213114754098361\n",
      "Decision Tree Accuracy after removing column, chol  : \n",
      "0.7213114754098361\n",
      "Decision Tree Accuracy after removing column, fbs  : \n",
      "0.6885245901639344\n",
      "Decision Tree Accuracy after removing column, restecg  : \n",
      "0.7049180327868853\n",
      "Decision Tree Accuracy after removing column, thalach  : \n",
      "0.6721311475409836\n",
      "Decision Tree Accuracy after removing column, exang  : \n",
      "0.7704918032786885\n",
      "Decision Tree Accuracy after removing column, oldpeak  : \n",
      "0.7540983606557377\n",
      "Decision Tree Accuracy after removing column, slope  : \n",
      "0.7377049180327869\n",
      "Decision Tree Accuracy after removing column, ca  : \n",
      "0.7377049180327869\n",
      "Decision Tree Accuracy after removing column, thal  : \n",
      "0.6721311475409836\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "for column in data.columns:\n",
    "    if column != 'target':\n",
    "        print(\"Decision Tree Accuracy after removing column,\", column, \" : \")\n",
    "        print(get_decision_tree_accuracy_without_column(column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that by removing the exang feature from our dataset we can reach better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - 4 ) Model with 5 Random Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Random.sample of <random.Random object at 0x1750340>>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "temp_columns = list(data.columns)\n",
    "temp_columns.remove('target')\n",
    "temp_columns\n",
    "random.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns inlcluded in dataset for training the tree :\n",
      "['trestbps', 'chol', 'thal', 'slope', 'sex', 'target']\n",
      "Accuracy: 0.639344262295082\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "temp_columns = list(data.columns)\n",
    "temp_columns.remove('target')\n",
    "columns = random.sample(temp_columns, k = 5)\n",
    "columns.append('target')\n",
    "data = data[columns]\n",
    "Y = data.target\n",
    "X = data.drop(columns=['target'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"columns inlcluded in dataset for training the tree :\")\n",
    "print(columns)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - 5 ) Random Forest with Random Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_features():\n",
    "    data = pd.read_csv('data.csv')\n",
    "    temp_columns = list(data.columns)\n",
    "    temp_columns.remove('target')\n",
    "    columns = random.sample(temp_columns, k = 5)\n",
    "    columns.append('target')\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_accuracy_with(columns):\n",
    "    data = pd.read_csv('data.csv')\n",
    "    data = data[columns]\n",
    "    Y = data.target\n",
    "    X = data.drop(columns=['target'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "    Xs_train = [X_train.sample(n = 150) for i in range(5)]\n",
    "    ys_train = [y_train[x_train.index] for x_train in Xs_train]\n",
    "    clfs = [DecisionTreeClassifier() for i in range(5)]\n",
    "    fitted_clfs = [clfs[i].fit(Xs_train[i], ys_train[i]) for i in range(5)]\n",
    "    preds = [fitted_clf.predict(X_test) for fitted_clf in fitted_clfs]\n",
    "    final_preds = []\n",
    "    for i in range(len(X_test)):\n",
    "        ones = [pred[i] for pred in preds].count(1)\n",
    "        zeros = [pred[i] for pred in preds].count(0)\n",
    "        if zeros < ones:\n",
    "            final_preds.append(1)\n",
    "        else:\n",
    "            final_preds.append(0)\n",
    "    return metrics.accuracy_score(y_test, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features included in the Model: \n",
      "['restecg', 'slope', 'chol', 'sex', 'cp', 'target']\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "temp_columns = list(data.columns)\n",
    "temp_columns.remove('target')\n",
    "columns = random.sample(temp_columns, k = 5)\n",
    "columns.append('target')\n",
    "print(\"features included in the Model: \")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest Accuracy: \",get_random_forest_accuracy_with(columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bootstraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a standard training set  D  of size n, bagging generates m new training sets  Di , each of size n′, by sampling from D uniformly and with replacement. By sampling with replacement, some observations may be repeated in each  Di . If n′=n, then for large n the set  Di  is expected to have the fraction (1 - 1/e) (≈63.2%) of the unique examples of D, the rest being duplicates. This kind of sample is known as a bootstrap sample. Then, m models are fitted using the above m bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n",
    "\n",
    "It reduces variance and helps to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data. In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set.\n",
    "\n",
    "Bagging reduces variance and helps to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests differ in only one way from bagging: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\".\n",
    "\n",
    "The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this data set and the numbers we used for creating subtrees were not perfect, and considering the fact that running the program again and again would give us different results because of it's random nature, we can see that usually, bagging accuracy is higher than simple decision tree since it stops overfitting. And we can also conclude that random forest can be better than both of the decision tree and bagging because it has not just the benefits of bagging, but also can prevent correlation between trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
